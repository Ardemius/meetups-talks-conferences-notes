= Vendredi 11:15 / 12:00 - Stream All Things - Patterns of Modern Data Integration
Thomas SCHWENDER <https://github.com/ardemius[@ardemius]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ../images
:source-highlighter: highlightjs
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
//:toclevels: 3
// To turn off figure caption labels and numbers
:figure-caption!:

toc::[]

Par *Gwen Shapira*, *Confluent*, author of "Kafka - The Definitive Guide” and "Hadoop Application Architectures". +
https://cfp.devoxx.fr/2018/talk/NCH-3345/Stream_All_Things_-%5FPatterns_of_Modern_Data_Integration[Descriptif de la conférence sur le site du CFP de Devoxx] +
icon:tags[] Key words : Kafka, Stream Processing, Event-Driven Microservices

ifdef::env-github[]
https://www.youtube.com/watch?v=vfFmcIlKo_M[vidéo de la présentation sur YouTube]
endif::[]
ifdef::env-browser[]
video::vfFmcIlKo_M[youtube, width=640, height=480]
endif::[]

== Overview

====
80% of the time in every project is spent on data integration: Getting the data you want the way you want it. +
This problem remains challenging despite 40 years of attempts to solve it. +
We want a reliable, low latency system that can handle varied data from wide range of data management systems. +
We want a solution that is easy to manage and easy to scale. Is it too much to ask?

In this presentation, we’ll discuss the basic challenges of data integration and introduce design and architecture patterns that are used to tackle these challenges. +
We will explore how these patterns can be implemented using Apache Kafka and share pragmatic solutions that many engineering organizations used to build fast, scalable and manageable data pipelines.
====

== Notes

.Comparison of advantages and drawbacks for DWH, datalake and *Data Stream*
image::stream-all-things_01.jpg[]

Gwen doesn't like Informatica !

image::stream-all-things_02.jpg[]

* DWH and datalake: for *ETL engineers*
* datalake and data stream: for *software engineer*

-> Now, *software engineers replaced a whole bunch of other jobs*:

image::stream-all-things_03.jpg[]

They do things a bit differently:

* write code, rather than be the "Informatica Guy"
* build apps, rather than helping people to explore data

In fact, they have *multiple responsibilities* (Swiss knife)

=== Stream of events

Main pain-points of *data integration*:

* all your data is event streams : continuous chain of events, the whole being *immutable*

Data integration is to *collect*, *process* and *store data*.

Managing hundreds of different event types is insane. +
-> a solution: *put all streams in ONE Kafka*

Put everything in one place is a good pattern -> it avoids the need of connecting to hundreds of different stores

.Recap
image::stream-all-things_04.jpg[]

La structure de log de Kafka est un gros avantage qui permet à Kafka de scaller facilement.

In Kafka, producers are totally unaware of what the consumers do with the data. +
This can be a problem (you can easily break a lot of consumers with broadcasting a bad change)

Exemple

image::stream-all-things_05.jpg[]

[IMPORTANT]
====
Stream processing world -> *Event schemas ARE the API*

image::stream-all-things_07.jpg[]
====

image:stream-all-things_06.jpg[]
image:stream-all-things_08.jpg[]

.A reminder of Kafka scalable model
image::stream-all-things_09.jpg[]

*Hipster Stream Processing* : a very simple framework. +
"very simple" meaning *NOT* for all use cases

.Kafka Connect
image::stream-all-things_10.jpg[]

A (very) big bunch of connectors exist to connect Kafka to sources and sinks.

WARNING: Don't take this pattern too far! +
Too far meaning *state management*

=== Example of Kafka workflow

.Initial workflow
image::stream-all-things_11.jpg[]

. *Update 1*
+
image::stream-all-things_12.jpg[]
+
-> But *latency problems* with the connection with the DB (not enough throughput) 
+
. *Update 2* +
On ajoute un cache pour éviter les problèmes de latence côté BDD.
+
image::stream-all-things_13.jpg[]
+
-> Mais vient alors le pb : *quand dois-je invalider le cache ?*
+
. *Update 3* +
We store the event log from the DB in Kafka -> we do CDC (Change Data Capture)
+
image::stream-all-things_14.jpg[]

En conclusion, *we turn state (in DB) into stream of events (in Kafka)*

image:stream-all-things_15.jpg[] image:stream-all-things_16.jpg[]

Dans les faits, il y a plusieurs grandes questions à se poser :

image::stream-all-things_17.jpg[]

* Si cela ne tient pas en mémoire, je dois sharder
* Maintenir un cache distribué n'est pas simple
* Comment persister l'état ? (avec la résilience qui va avec ?)
* *Comment gérer des co-partitions pour les JOIN ?*

-> Pour tout cela, il y a *KafkaStreams* +
Pour un exemple, voir https://www.confluent.io/blog/distributed-real-time-joins-and-aggregations-on-user-activity-events-using-kafka-streams/

== Avis

Gwen parle plus vite que Lucky Luke ne tire... +
Mais la conférence était très intéressante, avec une bonne illustration d'un workflow Data stream (utilisant Kafka bien sûr) et des difficultés probablement rencontrées.
