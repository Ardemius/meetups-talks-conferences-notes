= Jeudi 16:45 / 17:30 - Comment nous avons construit un Data Lake AWS de 700 To, performant et scalable
Thomas SCHWENDER <https://github.com/ardemius[@ardemius]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ../images
:source-highlighter: highlightjs
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
//:toclevels: 3
// To turn off figure caption labels and numbers
:figure-caption!:

toc::[]

Par Arnaud Milleker et Simon Cocula, tous deux travaillant chez Ogury. +
https://cfp.devoxx.fr/2019/talk/OYU-8905/Comment_nous_avons_construit_un_Data_Lake_AWS_de_700_To%2C_performant_et_scalable[Descriptif de la conférence sur le site du CFP de Devoxx] +
icon:tags[] Key words : Amazon Web Services, Big Data

ifdef::env-github[]
https://www.youtube.com/watch?v=cJE8ihPGB74&list=PLTbQvx84FrARfJQtnw7AXIw1bARCSjXEI[vidéo de la présentation sur YouTube]
endif::[]
ifdef::env-browser[]
video::cJE8ihPGB74[youtube, width=640, height=480]
endif::[]

== Overview

====
*A l'origine était le Data Warehouse* +
Ogury a commencé à entreposer ses Data "propres" dans Redshift (Data Warehouse d'AWS) en 2016. Le besoin est simple : centraliser les données pour voir et comprendre ce qu'il se passe opérationnellement sur le targeting publicitaire de nos millions de téléphones en services.

*Le Data Lake efficace* +
2 ans plus tard, nous avons des centaines de millions de téléphones en service, alimentant de la Data pour des usages bien différents (Data Scientist, Business Analysts, ...). Nous avons maintenant un Data Lake de 700 To, basé sur Redshift, Spectrum, Athena, Data Pipeline, Spark. Il est performant, scalable et répond à tous les besoins :
- depuis le Business Analysts qui veut un accès très simple
- au Data Scientist qui veux des accès à tout et partout ... Perfs incluses !
- tout en évitant le Data swamp ou manque d'organisation

Nous verrons ensemble quelles sont les principales composantes de notre proposition de Data Lake AWS, leur fonctionnements, leur optimisations et limitations. Cette présentation est particulièrement adaptée aux Data Engineers, Tech Lead, Architectes ... Ou curieux !
====

== Notes

image::datalake-AWS-700To_01.jpg[]
image::datalake-AWS-700To_02.jpg[]
image::datalake-AWS-700To_03.jpg[]
image::datalake-AWS-700To_04.jpg[]
image::datalake-AWS-700To_05.jpg[]
image::datalake-AWS-700To_05.jpg[]
image::datalake-AWS-700To_06.jpg[]
image::datalake-AWS-700To_07.jpg[]
image::datalake-AWS-700To_09.jpg[]
image::datalake-AWS-700To_10.jpg[]
image::datalake-AWS-700To_11.jpg[]
image::datalake-AWS-700To_12.jpg[]
image::datalake-AWS-700To_13.jpg[]



