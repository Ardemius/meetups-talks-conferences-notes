= 2024/03/21 - Groupe OnePoint - Data & IA Summit
Thomas SCHWENDER <icon:github[] https://github.com/Ardemius/[GitHub] / icon:twitter[role="aqua"] https://twitter.com/thomasschwender[@thomasschwender]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:resourcesdir: ./resources
:source-highlighter: highlightjs
:highlightjs-languages: asciidoc
// We must enable experimental attribute to display Keyboard, button, and menu macros
:experimental:
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
//:sectnums:
// Add an anchor with hyperlink before the section title
:sectanchors:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

Talk organisé par *OnePoint* dans les locaux 29 rue des Sablons à Paris.

* Plan et programme du salon : https://www.groupeonepoint.com/fr/notre-actualite/programme-plan-data-ia-summit-2024/ +
image:https://www.groupeonepoint.com/wp-content/uploads/2024/03/dia_planning2024_A1.jpg[]

== Keynote d'ouverture

* Avec Gontran PEUBEZ, partner OnePoint

== 09h15 - 10h00 : Data et IA issue and trends

Présenté comme l'année passée par *Nora Diep* (spécialisée sur le Data Empowerment), *Jamila Yahia* et *Fanny Brottes*, toutes leaders Onepoint (département Data, données marketing)

*Data issues* 

    * gestion des données

    * people
        ** du fait des difficultés finalement habituelles de recrutement, *l'accent est de nouveau mis sur la formation* interne des collaborateurs (sur l'IA)
            *** et qui dit formation dit risque de départ du prestataire si c'est le cas, D'OU des effectifs qui vont probablement pencher davantage du côté d'une population d'*ITs internes* 

    * industrialisation

*Data trends*

image:20240321_data-ia-summit_01.jpg[]

    * *Data Academy* : c'est bon, c'est fait, on a formé c'est devenu du BAU
    * *Data Office as Profit Center* : malheureusement pas dans l'air du temps même si pourrait être tellement utile. Toujours perçu comme un centre de coût, et non un apport de valeur

    * *Data Productization* : 

        ** value driven data governance
            *** la "reprise en main" des data par les métiers est un grand axe, avec les *data products* comme l'un des grands principes sous-jacents.
                **** Toujours avec la complexité d'identifier les data dont vont être composés les data products

        ** low code data platform

        ** beyond 360° Data
            *** "360°" : les données qu'on "a", c'est le 1er niveau
            *** les 2 autres niveaux sont le *720*, les données que je vais pouvoir échanger avec d'autres dans un environnement sécurisé, et le *1080*, tout ce qui va venir enrichir on top des données 720 : le 3rd party cookie par exemple
            

        ** solutions spécialisée par métier, par secteur -> la "game changer" est vraiment la *spécialisation*
        ** avec une prise en main facilité par les utilisateurs

    * *Protection, Trust, Global Responsibility* : 

        ** SecNumCloud

        ** Trustworthy AI
            *** Intro de l'IA Act : avoir une IA de confiance "centrée sur l'homme"
            *** l'humain doit garder le contrôle

        ** *RESET* : Renforcement de la Sécurité et de l'Ethique des Technologies +
        RESET est un acronyme français qui signifie "Renforcement de la Sécurité et de l'Ethique des Technologies". Il s'agit d'un programme gouvernemental français lancé en 2021 visant à renforcer la confiance dans le numérique et à protéger les données personnelles et composé de 5 axes principaux : +
            *** Lutte contre la cybercriminalité
            *** protection des données personnelles
            *** développeemnt d'une IA éthique
            *** renforcement de la confiance dans le numérique
            *** coopération internationale

    * *"DAC" engineer*
        ** une évolution du data engineer : monte les tuyaux pour l'échange de données, en lien avec les interactions par API qui ont explosé
        ** "A" pour "API"
        ** "C" pour "Cloud"

    * Centaure vs Cyborg

        ** qu'est-ce qui doit être mis dans les mains de la machine, et qu'est-ce qui doit rester dans les mains des humains
        ** *Centaure* : vont utiliser l'IA de façon complètement conscience et vont lui confier des tâches pour lesquelles elles sont les plus efficaces
        ** *Cyborg* : vont utiliser l'IA sans même s'en rendre compte car l'IA va être présente partout dans nos vies.

.Contact avec les intervenantes et IA Clinic
image:20240321_data-ia-summit_02.jpg[]

== 10h15 - 11h00 : Quels sont les défis majeurs de l'éducation à l'ère de l'intelligence artificielle. 

Avec Thierry Coulhon, Polytechnique , Romain Soubeyran, Mines Paris Tech, Alice Guilhon, Skema, Muriel Touaty, Partner Onepoint (Atelier)

.Conférenciers
image:20240321_data-ia-summit_03.jpg[]

* Comment préparer les apprenantes à un monde où les machines peuvent leur être supérieures ?
* Comment former des individus résilient et éthiques ? 

* Comment former les managers pour qu'ils puissent parler / comprendre la langue de l'IA ?
    ** les tech la parlent, mais ne parlent pas ou pas bien le langage des managers
    ** centre d'*IA for business* installé par Skmea à Montreal, car ils auraient trouvé là-bas un "lieu acculturé" où l'on parle les 2 langages. 
    ** Skmea : 8 sites, 10 000 étudiants et beaucoup de nationalités différentes
    ** il y a de grosses différences dans la façon d'apprendre suivant les pays (Brésil, Chine, etc.)

* Habituellement, on a tendance à dire que les disruptions ont moins d'impact que l'on le pense à court terme, et plus à long terme
    ** Dans le cas présent, avec l'IA, l'impact est parti pour être très fort même dès le court terme

* On constate qu'aussi bien le groupe "Institut polytechnique" que Centrale Supélec cherche à créer des centres de compétences regroupant leurs différentes écoles

* Les disciplines sont de plus en plus hybrides (hybridation des compétences), on n'a plus le "matheux dans sa tour d'ivoire" qui peut bosser seul dans son coin. +
Dans notre quotidien, les matières ont de plus en plus besoin d'être interconnectées pour répondre à nos besoins. Ou plutôt que de véritables "besoins", disons que nos usages se reposent de plus en plus sur un grand nombre de disciplines / matières et non 1 seul et unique comme c'était beaucoup plus le cas avant
    ** D'où une *explosion de l'hybridation des compétences* dans les enseignements de nos écoles

* Alice sur ce qu'elle a vécu avec Skmea : "quand on est en France et qu'on est pas la 1ere école de France, on ne vous parle pas". Skmea est la 5 ou 6e (dixit Alice), donc on ne voulait pas nous parler / nous entendre
    ** DONC Skmea part au Brésil où la mentalité serait plutôt "Ah vous proposez un truc innovant, ben venez donc !"
    ** plus évidemment la légendaire lourdeur de l'administration française...

    ** et quand dans l'enseignement on est présent dans plusieurs pays, on peut plus facilement "passer outre" les interdits d'un pays en particulier

== 11h15 - 12h00 : Challenges de l'IA à l'échelle : Simplifier votre Ecosystème Data pour Amplifier les bénéfices de l'IA

Avec Pierre Maussion, Teradata Solution Engineers, Denis Molin, Teradata Principal Data Scientist (Atelier)

* Partenariat entre Onepoint et Teradata
    ** Onepoint supporte leur dernière offre *VantageCloud lake*

* *Intérêt de l'hybridation* avec cloud et on-premise : on gère plus facilement les données sensibles ne pouvant PAS être placées dans le Cloud.

* Teradata est très cher, MAIS *VantageCloud lake serait apparemment très compétitif* comparé aux offres des Cloud providers

* Teradata pense être parmi les mieux placés pour la mise en place de data mesh et autres data products

.Teradata VantageCloud main components
image:20240321_data-ia-summit_04.jpg[]
image:20240321_data-ia-summit_05.jpg[]
image:20240321_data-ia-summit_06.jpg[]

* On voit que VantageCloud Lake (dernière image) s'appuie sur les principes actuels d'un Cloud data lakehouse avec une persistance s'appuyant sur les formats ouverts (Iceberg et autres)

image:20240321_data-ia-summit_07.jpg[]

* *Teradata QueryGrid* : un Data fabric permettant de relier, de faire communiquer la technologie Teradata avec les technologies partenaires
    ** Cela permet à Teradata d'être hybride et multi-cloud

.les principaux différenciants de VantageCloud Lake D'APRES Teradata
image:20240321_data-ia-summit_08.jpg[]

.L'évolution de Teradata vers l'IA
image:20240321_data-ia-summit_09.jpg[]

Conclusion : 

    * Teradata veut intégrer la liste des éditeurs de Cloud data lakehouse "qui comptent".
    * C'est une solution qui pourra intégrer la liste des produits à comparer / tester pour nos études

== 12h15 - 13h00 : Entre tradition et innovation : L'IA comme accélérateur de la modernisation de vos plateformes

Avec Thomas Rudel, Dataiku account executive, Nicolas Willems, Leader, Onepoint (Espace Workshops)

* Nicolas : par rapport à la transformation en cours de SAS et son changement de licensing, Onepoint a reçu de nombreux sollicitations, questions et inquiétudes de ses clients. +
-> Et Onepoint a décidé de répondre à ces questions à l'aide de l'IA Gen, ce pourquoi ils se sont également rapprochés de leur partenaire Dataiku

* 40 certifiés Dataiku chez Onepoint, dont une bonne partie pouvant eux-mêmes dispenser des formations Dataiku

* On peut considérer que Dataiku est un studio de développement de projet Data, NON verticalisé (ils travaillent avec tous les secteurs d'activité)
* Dataiku : approche de bout en bout de traitement de la data

image:20240321_data-ia-summit_10.jpg[]

.Répartition des différents types d'utilisateurs de Dataiku
image:20240321_data-ia-summit_11.jpg[]

* La *valeur de la plateforme* : 
    ** rapidité et efficacité
    ** efficacité technique : baisse du coût par projet à l'aide de cette plateforme de bout-en-bout qui nécessite moins de dépenses d'intégration
    ** contrôle : gouvernance de la donnée et gestion du risque
        *** TOUS les projets sont sécurisés

.Les intérêts de Dataiku pour moderniser sa plateforme Data
image:20240321_data-ia-summit_12.jpg[]

REX d'une mission chez un assureur : sortir de SAS pour aller sur Dataiku

.Contexte et objectifs
image:20240321_data-ia-summit_13.jpg[]

* Recensement : 40 000 assets et 80 To de data
* un SAS qui était détourné de son usage 1er (SAS est un outil de stat à la base) pour souvent servir à de la préparation de data
* cible de la migration : Dataiku + Tableau + BI

.Stratégie pour la migration
image:20240321_data-ia-summit_14.jpg[]

    1. xxx
    2. Comment je transforme ?
    3. yyy
    4. Conduite du changement : avec des experts SAS proches de la retraite et qu'il faut complètement faire changer de stack technologique

* Développement d'un *parser des programmes SAS* pour évaluer la complexité de ces derniers 
    ** -> cela alimente la stratégie de migration : on commence par les plus simples ou les plus compliqués

.Macro-architecture
image:20240321_data-ia-summit_15.jpg[]

* On ne veut pas mettre les programmes et les données sur le Cloud (Google), seuls les scripts y transitent.

.Principe de la transformation
image:20240321_data-ia-summit_16.jpg[]

* pour transformer un programme SAS de plusieurs milliers de lignes, on va splitter le code en petits bouts que l'on pourra plus facilement valider / vérifier

* Au final pour ce projet Onepoint et Dataiku ont développé un outil d'assistance à la migration

.Résultat brut de la migration d'un programme complexe (script de 4000 lignes)
image:20240321_data-ia-summit_17.jpg[]

Les avantages de la démarche : 

    * *optimisation de l'effort de migration* : x3 d'accélération
        ** Thomas : sur la phase 2 de découpage du script en petits bouts, ces derniers sont-ils validés automatiquement ou par revue manuelle ?
            *** Nicolas : une recette *globale* est effectuée automatiquement : on vérifie qu'on obtient exactement la même table que le programme SAS avec la nouvelle solution. +
            MAIS, si ça plante, pas de magie, il faut manuellement aller mettre la main dans le script SAS original (et pleurer...)
            *** donc pour la migration d'un programme complexe, on peut se poser la question de l'intérêt de l'usage de l'outil, car on sait qu'on va devoir "mettre les mains dedans". +
            -> Mais déjà, on peut se dire que l'outil a un réel intérêt pour les 80% des cas les plus simples, pour lesquels on profite pleinement de cette automatisation. +
            Pour les 20% les plus durs restant, ce sera de toute façon la misère, il faudra mettre les mains dedans.

    * des utilisateurs acteurs de leur migration et qui s'approprient la cible
    * l'opportunité de reconsidérer les programmes prioritaires
        ** avec pas mal de programmes obsolètes supprimés
    * Attirer les talents en proposant une offre de service ouverte et collaborative

Q&A : 

* Thomas Rudel "l'avantage de Dataiku : il est agnostique"
    ** Thomas : mais comme tous les éditeurs de solutions de Cloud data lakehouse, c'est l'un des principaux intérêts de ces solutions

* Nicolas : c'est SENS (Cloud "dit souverain") backé par Google qui a été utilisé pour la cible.

* Aller à l'IA Clinic, demander Philippe et des infos sur l'outil *OneMig*, *outil de migration de code* en cours de développement chez Onepoint

== 13h30 - 14h00 : AI Clinic

* J'ai demandé son avis au "docteur" si l'évolution du marché quant aux RAG à la demande : make or buy ? On attend des solutions plus "finies / élaborées" de la part des éditeurs, ou on reste sur des développements internes, ou un mixte des 2 ?
    ** on partage le même avis le docteur et moi : +
    -> Tant qu'il n'y a pas de besoin critique ON ATTEND, MAIS pas s'en rien faire : on se forme, on lance des projects exploratoires pour bien se faire la main à la techno

== 14h00 - 14h45 : IA : au-delà de l'avancée technologique, le défi politique. 

Avec Frédérique Vidal, Ministre de l'Enseignement supérieur, de la Recherche et de l'Innovation de 2017 à 2022

* Frédérique : on régule "avant de faire" ou on "laisse aller" ? C'est la grande question.
* Encore plus essentiel que de former les étudiants en maths et physique POUR L'IA, il faut avant tout former la population
    ** trop de personnes pensent que la machine ne se trompe jamais
    ** et que la machine est plus intelligente qu'eux

* les IAs n'ont aucune idée de la "signification" des données qu'elles analysent
    ** Ce sont des machines probabilistes qui renvoient la réponse la plus probable

* On a des jeunes brillants, qui n'hésitent pas à créer leur startup, qui arrivent à trouver des fonds en 1ere ou 2e levée, MAIS ça coince après quand on veut vraiment se développer. +
-> A partir de là, à toutes les autres levées, les fonds ne sont plus français.

* Pour toute réponse, la bonne réponse se situe à l'*échelle européenne*.

* Il s'agit vraiment (aux yeux des personnes présentes) de la *4e révolution industrielle*.
* Pensez l'IA comme un outil, et pas comme un prescripteur de quoi que ce soit
    ** et on ne va PAS mettre de l'IA partout

== 15h00 - 15h45 : Personnalisation et IA : L'alliance puissante de Tealium pour un engagement client incomparable

Avec Pascal Morvan, Tealium Senior Solution Consultant et Anne-Sophie Dutroncy, Associate Onepoint. (Espace Workshops)

* Pascal : "Tealium permet de connecter vos données pour que vous puissiez vous connecter à vos clients"
* Pascal : Tealium dans toutes les verticales, tous les secteurs d'activité
* Tealium est une CDP, une Customer Data Platform

* Le Gartner a sorti dernièrement sa toute 1ere étude sur le CDP (Customer Data Platform) et Tealium est en tête de liste

.Quelques constats et préoccupations sur l'IA
image:20240321_data-ia-summit_18.jpg[]

    * La conformité
    * les silos de données : comment les réconcilier pour en permettre l'usage par l'IA
    * le cookie est mort (ou presque), et maintenant / après ?
    * L'IA exige de la data en temps réel
        ** Thomas : souvent vrai, MAIS dépend quand même des use cases

.Les besoins, conséquents et variés, de l'IA
image:20240321_data-ia-summit_19.jpg[]

.L'un des gros intérêts de Tealium : libérer du temps pour les Data scientists
image:20240321_data-ia-summit_20.jpg[]

* Le même constat que d'habitude : les Data scientists "perdent" énormément de temps pour préparer les données
    ** -> Tealium permet de gagner de beaucoup de temps à ce niveau

* Pascal : *"on doit travailler la donnée au moment où on la connecte"*
    * Thomas : je comprends le point MAIS ce n'est "qu'un avis" : on pourrait faire différemment (apprentissage non supervisé avec découverte des catégories *après coup*)

* *Tealium apporte des données consenties*, filtrées, organisées et enrichies aux modèles d'IA et récupère les scores des modèles à chaque outil et point de contact, en temps réel.
    ** Thomas : Tealium travaille vraiment en lien particulier avec la CDP. +
    La "récupération des scores des modèles" est une expression un rien traître, il s'agit en fait de la récupération des résultats de la campagne marketing associée à la CDP : ai-je réussi à bien toucher mes cibles / clients ? L'activation a-t-elle réussi ?

.Définition de l'activation dans le cadre d'un CDP
[NOTE]
====
L'*activation* est l'étape finale du cycle de vie des données clients dans un CDP. Il s'agit de l'utilisation des données clients collectées et organisées pour *générer des actions concrètes* et *influencer le comportement des clients*.
====

.Avantages de Tealium pour l'IA
image:20240321_data-ia-summit_21.jpg[]

* Dès la source, on s'est assuré du *consentement*

.Tealium pour permettre la collecte de données pour l'IA
image:20240321_data-ia-summit_22.jpg[]

.Macro architecture de Tealium
image:20240321_data-ia-summit_23.jpg[]

-> Tealium comme un dictionnaire de données, il permet de standardiser les données quelle que soit la source

* Thomas : Tealium, s'il permet de collecter et qualifier les data quelle que soit la source, ne pourrait-il pas faciliter la mise en place de data mesh ? (approche à laquelle je ne crois pas)

.Cas d'usage de Tealium
image:20240321_data-ia-summit_24.jpg[]

.Pourquoi les organisations d'IA choisissent-elles Tealium ? Pour ses certificats de sécurité
image:20240321_data-ia-summit_25.jpg[]
image:20240321_data-ia-summit_26.jpg[]

* Donc surtout pour des raisons de sécurité : on sent bien le *poids croissant de la régulation sur ces sujets*

== 16h00 - 16h45 : Innovation à l'échelle avec Microsoft x Onepoint : Solutions mises en œuvre en 2023 et Stratégies 2024

Avec Sandrine Tarnaud, Microsoft Head of Data and AI, François Binder, Partner Onepoint (Atelier)

* Les dernières transformations : Cloud -> Digital Transformation -> AI transformation
* Mais dans tous les cas, "tout" est devenu "data driven"

* D'ici à 2026 : 300 Md$ seront investis dans l'IA

.Intégration des technologies OpenAI dans Azure
image:20240321_data-ia-summit_27.jpg[]

.Toutes les applications Microsoft peuvent s'appuyer sur un Copilot
image:20240321_data-ia-summit_28.jpg[]
image:20240321_data-ia-summit_29.jpg[]

.La stratégie de Microsoft pour ces GenAI services
image:20240321_data-ia-summit_30.jpg[]

2 stratégies en fait : 

    * la partie de gauche *"Model as a Platform"*
    * et la partie de droit *"Model as a Service"* où en 3 clics avec AI Studio vous allez pouvoir gérer votre IA pipeline
        ** et Microsoft annonce déjà qu'ils sont convaincus que le marché va tendre vers cette 2e solution

.Les GenAI services ne sont qu'une partie des AI services de Microsoft
image:20240321_data-ia-summit_31.jpg[]

NOTE: Microsoft : le projet Trident est devenu le produit *Fabric*

.Le "voyage vers l'IA" et ses étapes
image:20240321_data-ia-summit_32.jpg[]

* -> S'y intègre les outils *OneLake*, *Fabric* et *Azure AI Studio*

* Etape 1 : Establish a *central repository for all data*
* Etape 2 : Prepare Data for AI innovation : improve the quality of your data to ensure it is complete, accurate and governed
* Etape 3 : Build generative AI experiences on top of your Data

.La confidentialité pour d'OpenAI SUR AZURE (au contraire d'OpenAI public)
image:20240321_data-ia-summit_33.jpg[]

.Comme beaucoup, Onepoint a créé son propre assistant pour ses effectifs
image:20240321_data-ia-summit_34.jpg[]

* Les principaux intérêts de NEO sont les couches de valeurs ajoutées dessus (c'est un RAG)

.Smart Data Management
image:20240321_data-ia-summit_35.jpg[]

* une solution en lien avec le Data Catalog de client
* *le LLM qualifie (tag) seul le caractère personnel d'une information* au regard du RGPD
* une solution "RAG like" utilisant Langchain pour la plomberie et générant la documentation des données qui sera insérée dans le Data Catalog
* Le constat de OnePoint sur cet outil, c'est simple "on va plus vite (x3) et ça coûte moins cher"

* Microsoft met en avant la *qualité des LLM de NVidia dans le cadre du codage*

Les étapes pour le passage à l'échelle de la genAI : 

    * il faut les bonnes compétences
    * il faut le bon accès à la data et qu'elle soit de qualité
    * il faut définir les outils pour gérer la continuité du service

.Gouvernance de l'AI
image:20240321_data-ia-summit_36.jpg[]

* Grrrrr... Microsoft pousse lui aussi l'approche Data Mesh...

.L'offre de GenAI et son vocabulaire
image:20240321_data-ia-summit_37.jpg[]

.Facteurs clé de succès
image:20240321_data-ia-summit_38.jpg[]
image:20240321_data-ia-summit_38.jpg[]

-> Dans tous les cas, le support du COMEX est absolument clé

== 17h00 - 17h45 : Les fondations d'un futur intelligent, en France

Avec Clément Morizot, Google Cloud Data & AI Specialist et Maxime Bourliatoux, Associate Onepoint (Atelier)

.La Data et l'IA au coeur des transformations du secteur banche / assurance
image:20240321_data-ia-summit_39.jpg[]

.Les expérimentations GenAI de Google avec ses clients Finance
image:20240321_data-ia-summit_40.jpg[]

Les fondations pour passer à l'échelle avec l'IA

    * 19% des compagnies pensent qu'elles utilisent au max leurs data
    * *L'accès à la Data reste LE problème principal*
        ** Clément : lors de son dernier projet, ils ont passé plus de temps à récupérer / déplacer les data que les travailler. +
        On récupère à droite, on déplace à gauche et c'est sans fin

* La conviction de Google : Une ligne métier doit pouvoir gérer ses data de façon autonome

.Une organisation "Data Inside"
image:20240321_data-ia-summit_41.jpg[]

.et des Data products (encore)
image:20240321_data-ia-summit_42.jpg[]

* Clément - Data products : ça peut être n'importe quoi (fichier ou autres) MAIS quelqu'un doit être responsable d'exposer quelque chose dans le temps

.Le use case de 2014 2015 avec la plateforme Hadoop centrale
image:20240321_data-ia-summit_43.jpg[]

* La BI a toujours été très difficile avec de l'Hadoop

.Et on a BigQuery qui "marche très bien"
image:20240321_data-ia-summit_44.jpg[]

* BigQuery permet de gérer use cases Big Data, analytics, ML
* BigQuery permet de "créer son data lake" SANS avoir besoin d'instancier un data lake : +
on utilise BigQuery pour intégrer les data petit à petit, et c'est lui qui permet de les mettre à disposition
    ** Thomas : une présentation très high level de BigQuery, attention, cela mérite plusieurs précisions 😉

.BigQuery couvre un peu tous les grands use cases de la Data ET gère les embeddings (et donc la GenAI)
image:20240321_data-ia-summit_45.jpg[]

.La stack technologique de Google Cloud AI
image:20240321_data-ia-summit_46.jpg[]

* Gestion multi-modèles : on peut déployer des modèles non Google sur l'infra de Google
* Boîte à outil disponible pour tuner les modèles

NOTE: rapport de 825 pages, soit ~900 000 tokens

* Avec Gemini et la prise en charge de son 1M de tokens, traiter un document de 1000 pages, Clément donne l'exemple d'un rapport financier, prend 7 sec pour une récupération d'infos ("trouve-moi l'info XXX" et fait une règle de 3 avec...)

.Côté sécurité, Google pousse SENS qui vise une certification SecNumCloud 3.2
image:20240321_data-ia-summit_47.jpg[]
image:20240321_data-ia-summit_48.jpg[]

NOTE: HSBC est le plus gros client GCP BigQuery d'Europe

Conclusion : 

    * Présentation très "Google number 1" mais les performances annoncées sont juste impressionnantes. +
    -> Chiffres à récupérer, beaucoup de slides dans la présentation, ce n'était pas fait pour une format 45 min.

Q&A : 

    * Thomas : vous pensez VRAIMENT avoir la certification SecNumCloud pour SENS ? +
    (Guillaume (Google) avait commencé la séance de Q&A en disant bien "qu'ils n'avaient RIEN à cacher" 😉)
        ** Guillaume (Google) est sûr à 100% que SENS aura le SecNumCloud 3.2 d'ici quelques semaines. +
        -> Ce sont des 100e de M€ d'investissement, on "ira jusqu'au bout"
            *** la demande a déjà été faite à l'ANSSI, il faut donc maintenant attendre la réponse de l'ANSSI sous quelques semaines (me semble court)

== 18h00 - 18h45 : IA Génératives : un séisme social culturel et civilisationnel

Avec Eric Sadin, philosophe Data & IA en écrivain

* Il est quand même très "clivant" / provoquant Eric Sadin

    ** "Yann LE CUN annonce que l'IA va permettre des progrès révolutionnaires dans la médecine" : Eric Sadin "Mais que connaît Yann LE CUN en médecine pour dire ça ?!"
        *** Thomas : de mon côté, vu les analyses permises par l'IA (analyse d'image pour la radiologie par exemple), je comprends les propos de Yann LE CUN, ça ne me choque pas

-> Discours très dense, écouter le replay, c'est très difficile à résumer


















