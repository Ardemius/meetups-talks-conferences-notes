= 2024/01/16 - Microsoft - Data & IA Innovation Day
Thomas SCHWENDER <icon:github[] https://github.com/Ardemius/[GitHub] / icon:twitter[role="aqua"] https://twitter.com/thomasschwender[@thomasschwender]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:resourcesdir: ./resources
:source-highlighter: highlightjs
:highlightjs-languages: asciidoc
// We must enable experimental attribute to display Keyboard, button, and menu macros
:experimental:
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
//:sectnums:
// Add an anchor with hyperlink before the section title
:sectanchors:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

== Agenda

* Voir Microsoft Event : https://msevents.microsoft.com/event?id=59699372

Vous avez √©t√© plus de 500 √† vous inscrire √† la premi√®re en janvier. Comme l'actualit√© dans l'IA et l'analytique est tr√®s riche ces derniers temps, nous vous invitons le 13 juin √† une nouvelle journ√©e de pr√©sentation et de d√©monstrations. A quelques jours des Jeux Olympiques notre devise est ¬´ Altius Citius Fortius ¬ª. 

Cette devise nous la d√©clinerons toute la journ√©e, et sp√©cialement le matin lors de la keynote afin de vous informer des derni√®res nouveaut√©s de nos offres Data & IA int√©gr√©es dans nos services ces derniers mois. 

L'apr√®s-midi sera consacr√©e √† des sessions workshops Analytics et IA durant lesquelles vous aurez l'opportunit√© de prendre en main nos technologies avec nos experts partenaires. 

Altius, plus vite avec la pr√©sentation des derni√®res annonces de l'√©v√®nement ¬´ Microsoft Build ¬ª et de la ¬´ FabCon ¬ª de Las Vegas. Nous reviendrons sur les derni√®res nouveaut√©s de Microsoft Fabric, que ce soient des annonces autour de la s√©curit√©, de l'int√©gration des sources de donn√©es, et bien √©videment des derni√®res surprises qui seront annonc√©es fin mai. 

Citius, plus haut avec une pr√©sentation par notre partenaire Nvidia de DGX Cloud dans Azure permettant de d√©velopper vos propres mod√®les d'IA g√©n√©rative. 

Fortius, plus fort avec des zooms sur les toutes derni√®res annonces de Microsoft dans le domaine de l'IA. Les IA g√©n√©ratives sont maintenant disponibles depuis plus d'un an dans Azure. Ce sera aussi l'occasion de parler industrialisation de ces solutions via la pratique LLMOps.

Programme : 

    * 09:30 AM - 10:00 AM : Accueil & Caf√©
    * 10:00 AM - 12:00 PM : Pl√©ni√®re innovation anim√©e par Eric Charbonnier, CTO Data & AI, Microsoft France
    * 12:00 PM - 01:30 PM : D√©jeuner & Networking
    * 02:00 PM - 05:00 PM : Ateliers & Demo (4 tracks) +
    image:20240613_Microsoft_Data-Innovation-Day_00.jpg[]
    * 05:00 PM - 06:00 PM : Cocktail & Networking

== Keynotes du matin

Intro par Eric Charbonnier, CTO Data & IA Microsoft

.La Microsoft Intelligent Data Platform
image:20240613_Microsoft_Data-Innovation-Day_01.jpg[width=800]

* Retour sur la derni√®re MS Build avec de nouveau un *Fabric* mis tr√®s en avant : +
image:20240613_Microsoft_Data-Innovation-Day_02.jpg[width=800]

* From Fabric to a "Fabric store" : donner la possibilit√© √† chacun de se cr√©er son propre workflow Fabric
    ** *Microsoft Fabric Workload Development Kit* +
    image:20240613_Microsoft_Data-Innovation-Day_03.jpg[width=800]

* Microsoft rappelle son fort partenariat avec *Databricks*, dont il utilise le format "Delta Parquet" (*Delta lake format*)
    ** ET grande annonce de build : le format *Iceberg* est maintenant √©galement support√© +
    image:20240613_Microsoft_Data-Innovation-Day_04.jpg[width=800]
* ET le partenariat de Microsoft avec *Snowflake* a √©t√© renforc√© : +
image:20240613_Microsoft_Data-Innovation-Day_05.jpg[width=800]
    ** Il ne sera plus n√©cessaire de dupliquer de la Data entre Fabric et Snowflake

* Comme vu dans les pr√©c√©dentes conf MS, les *Copilot* sont maintenant pr√©sents dans tous les produits Fabric : +
image:20240613_Microsoft_Data-Innovation-Day_06.jpg[width=800]
    ** et g√©n√©ration automatique du code SQL associ√© √† la question en NLP de l'utilisateur +
    image:20240613_Microsoft_Data-Innovation-Day_07.jpg[]

* Compliqu√© de b√¢tir aujourd'hui une *solution Temps R√©el*, qui implique de nombreuses technologies : +
image:20240613_Microsoft_Data-Innovation-Day_08.jpg[width=800]
image:20240613_Microsoft_Data-Innovation-Day_09.jpg[width=800]

* Avec Microsoft Fabric, on va pouvoir cr√©er en quelques minutes une solution d'analyse en TR +
image:20240613_Microsoft_Data-Innovation-Day_10.jpg[width=800]

*DEMO* : r√©cup√©ration d'infos en TR des panneaux solaires install√©s sur le toit de la maison du speaker üòâ +
image:20240613_Microsoft_Data-Innovation-Day_11.jpg[width=800]

* Nouveaut√© : d√©tection automatique d'anomalies (√† l'aide du langage KQL)

Pr√©sentation de Nicolas SAVIDES, *NVidia*, Microsoft Alliance Manager, EMEA

    * La *valeur de NVidia* est avant tout dans notre *plateforme logicielle*
        ** plus que dans notre hardware -> je n'aurais jamais cru que c'√©tait √† ce point
    * NVidia a plus d'ing√©nieurs logiciels que hardware
    * Mais comme tout le monde n'a pas forc√©ment envie de retourner coder sur des couches basses, NVidia a d√©velopp√© de nombreux frameworks.

* Il y a des cas o√π l'on ne peut pas faire se servir d'un mod√®le via un appel API, il faudra le faire tourner chez soi : +
image:20240613_Microsoft_Data-Innovation-Day_12.jpg[width=800]
image:20240613_Microsoft_Data-Innovation-Day_13.jpg[width=800]

* NIM : NVidia Inference Microservices

image:20240613_Microsoft_Data-Innovation-Day_14.jpg[width=800]

-> NVidia met maintenant tr√®s en avant son expertise en conseil pour d√©ployer des stacks IA sur son mat√©riel ET sa plateforme logicielle (*NVidia AI Entreprise*) +
image:20240613_Microsoft_Data-Innovation-Day_15.jpg[width=800]

L'id√©e de Nvidia est de *vous aider √† passer le plus rapidement possible en PROD* avec vos pipelines d'IA et avec le meilleur ROI : +
image:20240613_Microsoft_Data-Innovation-Day_16.jpg[width=800]

* L'id√©e des NIM est de garder une libert√© compl√®te sur son mode de d√©ploiement

* DGX Cloud : une version manag√©e NVidia dans le tenant NVidia.
    ** probl√©matique quant √† la transmission / r√©cup√©ration des donn√©es : comment r√©cup√©rer les donn√©es du client final ?
        *** C'est la probl√©matique actuelle de NVidia : comment par exemple r√©cup√©rer les donn√©es dans Fabric ?

* *Azure AI Studio* est la tour de contr√¥le de l'IA Microsoft pour le d√©veloppement d'applications : 
    ** API and model choice
    ** complete AI toolchain
    ** xxx

* *Model Catalog* dans Azure : le meilleur catalogue de foundation models sur Azure

* Nouveaut√© : mise √† disposition du *SLM Phi-3* en Model as a Service
image:20240613_Microsoft_Data-Innovation-Day_17.jpg[width=800]
    ** Phi-3 medium 14 Mds de param√®tres
    ** Phi-3 sera le mod√®le derri√®re la nouvelle gamme de PC Microsoft

* *Assistants API* : Azure OpenAI Service +
image:20240613_Microsoft_Data-Innovation-Day_18.jpg[width=800]
    ** Les Assistants API viennent aider les d√©veloppeurs en leur fournissant un set d'API que l'on va pouvoir tr√®s facilement connecter √† nos solutions

image:20240613_Microsoft_Data-Innovation-Day_19.jpg[]
image:20240613_Microsoft_Data-Innovation-Day_20.jpg[]

* L'assistant via le "code interpreter" va lui-m√™me chercher comment interpr√™ter le fichier donn√© 

* Eric : *Phi-3*, la plus petite version, ~2 Mds de param√®tres, a √©t√© install√©e sur son PC
    ** et son PC n'a pas de GPU
    ** exemple, qui prend 700Mo de m√©moire (√† v√©rifier) et a consomm√© 50% de CPU, le tout en d√©connect√© : +
    image:20240613_Microsoft_Data-Innovation-Day_21.jpg[]

Open AI : the evolution of LLMs

* talk de Katia G Guzman, solution Architect chez Open AI

.Evolution de GPT-3 √† GPT-4 : beaucoup plus de use cases de disponibles
image:20240613_Microsoft_Data-Innovation-Day_22.jpg[width=800]

.Et avec GPT-4o on ajoute la voix, l'image et la video
image:20240613_Microsoft_Data-Innovation-Day_23.jpg[width=800]

* GPT-4o : a step towards natural human-computer interaction
    ** avec une demo bluffante : 
        *** mod√®le multimodal par d√©faut : *audio et vid√©o*
        *** r√©ponse en *Temps R√©el*
        *** une conversation "naturelle" -> on revient sur l'orientation *anthropomorphique* de l'IA de nos jours
        *** On aura acc√®s aux fonctionnalit√©s de cette d√©mo d'ici quelques semaines

* Benchmark provenant de la *Chatbot Arena* : "LLM 6" +
image:20240613_Microsoft_Data-Innovation-Day_24.jpg[width=800]
    ** GPT-4o correspond ici √† "I'm also a good GPT2 chatbot" et est de loin le meilleur
    ** il est meilleur que GPT-4 turbo dans presque tous les cas ET il est moins cher

* "*GPT-4o* √† comparer √† l'intelligence d'un *enfant de 5 ans*"
    ** Katia : *on vise un niveau PHD* a plut√¥t court terme : *on raisonne ici en mois*

* Katia confirme que les prix de l'usage des mod√®les vont continuer de baisser +
image:20240613_Microsoft_Data-Innovation-Day_25.jpg[width=800]

* Orientation vers la *customisation des mod√®les* : une version fine-tun√©e d'un GPT-3.5 turbo pourra obtenir de meilleurs r√©sultats qu'un GPT-4 +
image:20240613_Microsoft_Data-Innovation-Day_26.jpg[width=800]

* *D√©veloppement de la multimodalit√©* : 
image:20240613_Microsoft_Data-Innovation-Day_27.jpg[width=800]
image:20240613_Microsoft_Data-Innovation-Day_28.jpg[width=800]
    ** pour le moment pour GPT-4o, l'output vers l'audio et la vid√©o n'est pas encore dispo dans l'API

* Katia : 
    ** "Don't build for what is available today but for what is coming"
        *** On est bien d'accord...
    ** "Don't let cost or latency be a blocker"

== Atelier 3 : Pratiques avanc√©es de l'IA G√©n√©rative

NIVEAU AVANC√â - PARTENAIRE CELLENZA

PROGRAMME

    * *Retour (Rapide) sur les nouveaut√©s "AI" de la build* : Azure Al Studio, mod√®les GPT-4 et phi-3, √©volutions de syst√®mes de s√©curisation du contenu.
    * Prompt Management : Ma√Ætriser un long prompt.
    * *Function calling* : Appeler une API avec un LLM (et d√©montrer de la conso de data relationnelle dans cette API ?).
    * *Web browsing* : int√©gration d'une recherche web.
    * *RAG vs in context* : Est-ce cost efficient d'avoir plus de token.
    * *Evaluation des LLMs* : Outils et m√©thodologie.
    * *Monitoring* : Suivi de l'utilisation des LLMs: co√ªt par utilisateur, par appel et mise en place de seuils (Quotas).
    * *Scalabilit√© et Gouvernance* : Comment utiliser l'APIM pour suivre, load balancer et g√©rer les quotas.

Intervenants : 

    * Nicolas ROBERT : Technical Officer Cloud AI solutions chez Cellenza, MVP Microsoft AI
    * xxx

NOTES :

* Cellenza "partenaire de l'ann√©e" de Microsoft sur l'IA
    ** Cellenza est apte √† faire passer des certifications pour le compte de Microsoft

*NOUVEAUTES DE LA BUILD*

image:20240613_Microsoft_Data-Innovation-Day_29.jpg[width=800]

* Explosion de l'Azure AI Studio qui devient "Generally available" : cet outil a pour vocation d'√™tre LE point d'entr√©e de tous les projets √† base d'IA de Microsoft
    ** possibilit√© de faire de l'√©valuation de mod√®le : est-ce que le mod√®le r√©pond pour de la vraie donn√©e ? Est-ce qu'il n'hallucine pas ?
        *** on va pouvoir faire de l'√©valuation manuelle, automatis√©e, et manuelle / aid√©e par l'IA (dernier point √† creuser)

.Les nouveaut√©s d'Azure AI Studio 
image:20240613_Microsoft_Data-Innovation-Day_30.jpg[width=800]

    * MaaS : Microsoft as a Service

image:20240613_Microsoft_Data-Innovation-Day_31.jpg[width=800]

.GPT-4o : nouveau mod√®le
image:20240613_Microsoft_Data-Innovation-Day_32.jpg[width=800]

    * plus rapide et moins cher qu'un GPT-4 Turbo
    * attention ! Nicolas explique que suivant les cas de figures, GPT-4 Turbo donnait des r√©sultats de meilleurs qualit√©
    * GPT-4o n'utilise PLUS de syst√®me d'OCR, et donne de tr√®s bons r√©sultats d'analyse d'images : +
    image:20240613_Microsoft_Data-Innovation-Day_33.jpg[width=800]
    * ET *GPT-4o est disponible depuis 2 jours en Europe via la r√©gion Su√®de !*

.Phi-3 et les SLM qui se d√©veloppent
image:20240613_Microsoft_Data-Innovation-Day_34.jpg[width=800]

    * Nicolas : aujourd'hui, la plupart de nos missions impliquent des LLM, mais c'est pas dit que ce soit toujours le cas fin 2024 (passage aux SLM ?)

*PROMPTING*

image:20240613_Microsoft_Data-Innovation-Day_35.jpg[width=800]

.Les probl√®mes avec les contextes longs
image:20240613_Microsoft_Data-Innovation-Day_36.jpg[width=800]

    * La performance des LLM dans les t√¢ches peut diminuer avec l'augmentation des informations bruyantes dans l'invite
    * Les LLM capturent mieux les informations pertinentes situ√©es au d√©but ou √† la fin du prompt, mais leur performance diminue si ces informations se trouvent au milieu de longs contextes

.Les solutions possibles √† ces probl√®mes
image:20240613_Microsoft_Data-Innovation-Day_37.jpg[width=800]

    * LLMLingua : projet d√©velopp√© par Microsoft, r√©duit la longueur des prompts pour les LLMs en utilisant une m√©thodologie grossi√®re √† fine. Il utilise un petit mod√®le de langue bien entra√Æn√©, tel que GPT-2-small ou LLaMA-7B, pour d√©tecter les tokens non importants dans le prompt, permettant une compression efficace.

.Comment fonctionne LLMLingua
image:20240613_Microsoft_Data-Innovation-Day_38.jpg[width=800]

    * papier de recherche : https://arxiv.org/pdf/2310.06839

* Thomas : question sur Mamba est les mod√®les s√©lectifs : plut√¥t 1 mod√®le s√©lectif que 2 mod√®les bas√©s sur du Transformer pour s√©lectionner les tokens √† conserver ?
    ** Le speaker n'√©tait pas au courant de l'existence des mod√®les Mamba

* Azure AI Search : supporte nativement du reranking de chunks via un 2nd mod√®le d√©di√©

.Exemple d'usage de LLMLingua
image:20240613_Microsoft_Data-Innovation-Day_39.jpg[width=800]

    * On peut le tester soi-m√™me sur Hugging Face : https://huggingface.co/spaces/microsoft/llmlingua-2

.Conclusion quand √† l'int√©r√™t et l'usage de LLMLingua : 
image:20240613_Microsoft_Data-Innovation-Day_40.jpg[width=800]

*FUNCTION CALLING*

* La technologie derri√®re les *agents* des pipelines d'IA

.D√©finition du Function calling
image:20240613_Microsoft_Data-Innovation-Day_41.jpg[width=800]

* Essentiellement, l'appel de fonction permet aux LLMs d'interagir de mani√®re transparente avec des outils et des APIs externes.
    ** Voir https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models

.Comment cela fonctionne
image:20240613_Microsoft_Data-Innovation-Day_42.jpg[width=800]

* Thomas : dans le cas de plusieurs Tools, comment g√©rer le chevauchement des tools ? 
    ** "Utilise ce tool pour r√©pondre √† des questions sur les voitures ?"
    ** "Utilise ce tool pour r√©pondre √† des questions sur les moteurs ?"
    ** REX Cellenza : pas de magie, tout est dans le prompt... 
        *** MAIS on peut essayer une approche "Chain of Thoughts" o√π on vient it√©rer sur les r√©sultats "√† chaque passage" : 
            **** 1er passage j'ai utilis√© l'outil A et les r√©sultats √©taient pas tops, donc je REESSAYE
            **** 2e passage, cette fois je vais utiliser l'outil B et vais obtenir de nouveaux r√©sultats, suivant si bons ou pas, je continue.
            **** Et ainsi de suite...
    
*WEB BROWSING*

* Tous les LLMs sont entra√Æn√©s sur des corpus de donn√©es avec des dates de cut-off : +
image:20240613_Microsoft_Data-Innovation-Day_43.jpg[width=800]

* Pour pallier ce probl√®me, on donne acc√®s √† la recherche web au mod√®le : +
image:20240613_Microsoft_Data-Innovation-Day_44.jpg[width=800]
image:20240613_Microsoft_Data-Innovation-Day_45.jpg[width=800]

* Pour s'interfacer avec Google Search et obtenir des SERP, on peut utiliser *Serper* : +
image:20240613_Microsoft_Data-Innovation-Day_46.jpg[width=800]

    ** "Search Engine Results Pages (also known as "SERPs" or "SERP") are Google's response to a user's search query"
    ** Serper permet de sp√©cifier les sites sur lesquels les recherches vont √™tre effectu√©es
    ** Mais attention, cela veut dire qu'on va ouvrir le LLM √† des donn√©es non v√©rifi√©es venant de l'ext√©rieur.
        *** Il va donc falloir v√©rifier ces data r√©cup√©r√©es (avec un couche de content safety et des prompts de s√©curit√© ? "Si tu ne trouves pas d'infos, n'en invente pas", etc.)

* Question : comment choisir entre les r√©sultats venant de la recherche documentaire et ceux venant de la recherche web ?
    ** Dans l'exemple donn√©, *aucun reranking n'a √©t√© effectu√©*
    ** MAIS Cellenza a d√©j√† travaill√© sur des projets o√π le reranking √©tait n√©cessaire et r√©alis√© par une brique d√©di√©e pour s√©lectionner / reranker les r√©sultats provenant des 2 types de recherches

.D√©fis RESOLUS par les longs contextes
image:20240613_Microsoft_Data-Innovation-Day_47.jpg[width=800]

.D√©fis RESTANT malgr√© les longs contextes
image:20240613_Microsoft_Data-Innovation-Day_48.jpg[width=800]

*EVALUATION D'UN LLM*

.Pourquoi devons-nous √©valuer un LLM ?
image:20240613_Microsoft_Data-Innovation-Day_49.jpg[width=800]

    * L'√©valuation des LLM consiste √† mesurer syst√©matiquement leur performance, fiabilit√© et efficacit√© pour informer les d√©cisions sur leur utilisation.

.Comment d√©finir une m√©trique d'√©valuation ?
image:20240613_Microsoft_Data-Innovation-Day_50.jpg[width=800]

    * Une m√©trique d'√©valuation des LLM √©value une application de LLM en fonction des t√¢ches pour lesquelles elle a √©t√© con√ßue (sachant qu'une application de LLM peut simplement √™tre le LLM lui-m√™me)

Le calcul des scores et les 2 grands types de m√©triques :

.Les m√©triques SANS d√©pendances d'IA (scoreurs statistiques)
image:20240613_Microsoft_Data-Innovation-Day_51.jpg[width=800]

.Les m√©triques AVEC d√©pendances d'IA (LLM-Evals)
image:20240613_Microsoft_Data-Innovation-Day_52.jpg[]

    * Et l'√©valuation va √™tre men√©e par un 2nd LLM
    * on retrouve ici le classique "donne un score entre 1 et 5"

.Exemple de prompt (G-Eval)
image:20240613_Microsoft_Data-Innovation-Day_53.jpg[width=800]

.Exemple de code (G-Eval) avec DeepEval
image:20240613_Microsoft_Data-Innovation-Day_54.jpg[width=800]

    * DeepEval est un framework d'√©valuation open source pour les LLMs.

.Conclusion sur les  m√©thodes d'√©valuation des LLMs
image:20240613_Microsoft_Data-Innovation-Day_55.jpg[width=800]

*MONITORING et GOUVERNANCE*

.LLM Monitoring
image:20240613_Microsoft_Data-Innovation-Day_56.jpg[width=800]

.Solution : Azure OpenAI Monitor
image:20240613_Microsoft_Data-Innovation-Day_57.jpg[width=800]

    * Attention ! Ce graphe est PAR RESSOURCE, et non par use case.
        ** Si plusieurs use cases utilisent la m√™me ressource, impossible de savoir qui utilise quoi

.Solution : tracker la consommation de tokens
image:20240613_Microsoft_Data-Innovation-Day_58.jpg[width=800]

    * ajout d'un flag "*token_usage*" : reli√© √† l'utilisateur

.Stream_options qui marche pas encore sur Azure (√† v√©rifier) donc usage de la librairie tiktoken
image:20240613_Microsoft_Data-Innovation-Day_59.jpg[width=800]

Observer davantage avec *Azure APIM* : +
image:20240613_Microsoft_Data-Innovation-Day_60.jpg[width=800]

* Avantage : l'APIM permet de s√©curiser la gestion de mes cl√©s d'API
    ** Plut√¥t que de voir les cl√©s d'API en dur dans le code...
    ** La cl√© qui va √™tre donn√©e par l'APIM au client est une cl√© de l'APIM (*cl√© de  souscription de l'APIM*) et PAS une cl√© d'API de mon service

.Exemple d'usage d'APIM
image:20240613_Microsoft_Data-Innovation-Day_61.jpg[width=800]

.Configuration de l'APIM pour avoir le monitoring (les logs)
image:20240613_Microsoft_Data-Innovation-Day_62.jpg[width=800]

.Mise en place de la nouvelle policy "azure-openai-emit-token-metric"
image:20240613_Microsoft_Data-Innovation-Day_63.jpg[width=800]
image:20240613_Microsoft_Data-Innovation-Day_64.jpg[width=800]

.Subscription ID de l'APIM √† aller chercher dans les custom dimensions
image:20240613_Microsoft_Data-Innovation-Day_65.jpg[width=800]
image:20240613_Microsoft_Data-Innovation-Day_66.jpg[width=800]

    * Toute la donn√©e pr√©c√©dente est rebalanc√©e dans un *Log Analytics workspace*

.D√©passer les limitations de tokens
image:20240613_Microsoft_Data-Innovation-Day_67.jpg[width=800]

    * load balancer ? Pas le job natif d'un APIM
        ** Passer par un API Gateway ? l'API Gateway n'est PAS multi-regions
        ** Utiliser Front Door ? xxx
        ** Mais on va pouvoir impl√©menter un comportement de load balancer dans l'APIM

    * APIM Premium : 1 instance, c'est ~2500‚Ç¨ / mois
        ** Si on passe en niveau "d√©veloppeur" √ßa chute √† 50‚Ç¨ / mois MAIS on n'a plus de SLA...

    * utilisation des *backend pools d'APIM* : 
        ** Round robin natif par d√©faut

        ** Exemple de la configuration "avant" les backend pools : +
        image:20240613_Microsoft_Data-Innovation-Day_68.jpg[width=800]
        image:20240613_Microsoft_Data-Innovation-Day_69.jpg[width=800]
        ** "je d√©finis des variables qui me permettent de r√©cup√©rer dans ma conf les diff√©rents backends possibles"
        ** Code 429 : j'ai atteint les quotas d'OpenAI

        ** Et maintenant AVEC les backend pools : +
        image:20240613_Microsoft_Data-Innovation-Day_70.jpg[width=800]

.D√©finiton d'un backend pool avec les services qui seront utilis√©s
image:20240613_Microsoft_Data-Innovation-Day_71.jpg[width=800]

Application de quotas : ma√Ætriser la consommation de ses utilisateurs en termes de token +
image:20240613_Microsoft_Data-Innovation-Day_72.jpg[width=800]

*FINE TUNING (PEFT)*

* *PEFT* : Parameter Efficient Fine Tuning

.Un plus petit mod√®le bien fine-tun√© peut √™tre plus performant qu'un plus gros mod√®le
image:20240613_Microsoft_Data-Innovation-Day_73.jpg[width=800]

.LoRA : Low-Rank Adaptation (LoRA)
image:20240613_Microsoft_Data-Innovation-Day_74.jpg[width=800]

    * Fine-tuner le mod√®le en ajoutant de nouveaux param√®tres susceptibles d'√™tre entra√Æn√©s








